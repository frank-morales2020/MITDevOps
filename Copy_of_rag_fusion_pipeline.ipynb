{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/MITDevOps/blob/master/Copy_of_rag_fusion_pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41a58112-8a18-49ad-9c8c-2088778613d0",
      "metadata": {
        "id": "41a58112-8a18-49ad-9c8c-2088778613d0"
      },
      "source": [
        "# RAG Fusion Query Pipeline\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/run-llama/llama-hub/blob/main/llama_hub/llama_packs/query/rag_fusion_pipeline/rag_fusion_pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "\n",
        "This notebook shows how to implement RAG Fusion using the LlamaIndex Query Pipeline syntax."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c5bdbf5-d525-42e2-ab4a-19234c023491",
      "metadata": {
        "id": "8c5bdbf5-d525-42e2-ab4a-19234c023491"
      },
      "source": [
        "## Setup / Load Data\n",
        "\n",
        "We load in the pg_essay.txt data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "20b91e78-d733-44dd-b9a2-7c6eab3aee3d",
      "metadata": {
        "id": "20b91e78-d733-44dd-b9a2-7c6eab3aee3d",
        "outputId": "358a7d5e-1b70-4196-e2c6-30233c16ef09",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-01-11 20:18:07--  https://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/paul_graham/paul_graham_essay.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 75042 (73K) [text/plain]\n",
            "Saving to: ‘pg_essay.txt’\n",
            "\n",
            "pg_essay.txt        100%[===================>]  73.28K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2024-01-11 20:18:07 (48.8 MB/s) - ‘pg_essay.txt’ saved [75042/75042]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/paul_graham/paul_graham_essay.txt' -O 'data/paul_graham/paul_graham_essay.txt' -O pg_essay.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1785ffb1-5fc2-4855-854a-238003ff848b",
      "metadata": {
        "id": "1785ffb1-5fc2-4855-854a-238003ff848b"
      },
      "outputs": [],
      "source": [
        "#added by Frank Morales(FM) 11/01/2024\n",
        "!pip install openai  --root-user-action=ignore\n",
        "!pip install llama_index phoenix pyvis network\n",
        "!pip install llama_hub\n",
        "!pip install colab-env --upgrade --quiet --root-user-action=ignore\n",
        "!pip install accelerate\n",
        "\n",
        "from llama_index import SimpleDirectoryReader\n",
        "\n",
        "reader = SimpleDirectoryReader(input_files=[\"pg_essay.txt\"])\n",
        "docs = reader.load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e3bc7134-a34b-41de-b7e6-c85a6e356b56",
      "metadata": {
        "id": "e3bc7134-a34b-41de-b7e6-c85a6e356b56"
      },
      "source": [
        "### [Optional] Setup Tracing\n",
        "\n",
        "We also setup tracing through Arize Phoenix to look at our outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "d32462fb-07e0-4e5d-871c-98a782363330",
      "metadata": {
        "id": "d32462fb-07e0-4e5d-871c-98a782363330"
      },
      "outputs": [],
      "source": [
        "#import phoenix as px\n",
        "\n",
        "#px.launch_app()\n",
        "#import llama_index\n",
        "\n",
        "#llama_index.set_global_handler(\"arize_phoenix\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a7c9299c-06d6-4710-afb0-3cc13761f358",
      "metadata": {
        "id": "a7c9299c-06d6-4710-afb0-3cc13761f358"
      },
      "source": [
        "## Setup Llama Pack\n",
        "\n",
        "Next we download the LlamaPack. All the code is in the downloaded directory - we encourage you to take a look to see the QueryPipeline syntax!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "f5143a5f-7cd3-4aac-99e7-a357f3239f03",
      "metadata": {
        "id": "f5143a5f-7cd3-4aac-99e7-a357f3239f03"
      },
      "outputs": [],
      "source": [
        "# Option 1: Use `download_llama_pack`\n",
        "# from llama_index.llama_pack import download_llama_pack\n",
        "\n",
        "# RAGFusionPipelinePack = download_llama_pack(\n",
        "#     \"RAGFusionPipelinePack\",\n",
        "#     \"./rag_fusion_pipeline_pack\",\n",
        "#     # leave the below line commented out if using the notebook on main\n",
        "#     # llama_hub_url=\"https://raw.githubusercontent.com/run-llama/llama-hub/jerry/add_query_pipeline_pack/llama_hub\"\n",
        "# )\n",
        "\n",
        "# Option 2: Import from llama_hub package\n",
        "from llama_hub.llama_packs.query.rag_fusion_pipeline.base import RAGFusionPipelinePack\n",
        "from llama_index.llms import OpenAI"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain --quiet\n",
        "!pip install accelerate --quiet\n",
        "!pip install transformers --quiet\n",
        "!pip install bitsandbytes --quiet"
      ],
      "metadata": {
        "id": "Z22ggU77KHIv"
      },
      "id": "Z22ggU77KHIv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ADDED By FM 11/01/2024\n",
        "#%pip install colab-env --upgrade --quiet --root-user-action=ignore\n",
        "#!pip install accelerate\n",
        "\n",
        "import torch\n",
        "from textwrap import fill\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "import colab_env\n",
        "import os\n",
        "\n",
        "access_token = os.getenv(\"HF_TOKEN\")\n",
        "\n",
        "from langchain.prompts.chat import (\n",
        "    ChatPromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    )\n",
        "\n",
        "from langchain import PromptTemplate\n",
        "from langchain import HuggingFacePipeline\n",
        "\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.schema import AIMessage, HumanMessage\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.document_loaders import UnstructuredMarkdownLoader, UnstructuredURLLoader\n",
        "from langchain.chains import LLMChain, SimpleSequentialChain, RetrievalQA, ConversationalRetrievalChain\n",
        "from transformers import BitsAndBytesConfig, AutoModelForCausalLM, AutoTokenizer, GenerationConfig, pipeline\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
        "\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    torch_dtype=torch.float16,\n",
        "    low_cpu_mem_usage=True,\n",
        "    trust_remote_code=True,\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=quantization_config\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True, padding_side=\"left\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "#from transformers import AutoTokenizer, MistralForCausalLM"
      ],
      "metadata": {
        "id": "lwRPZY0mJxUV"
      },
      "id": "lwRPZY0mJxUV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generation_config = GenerationConfig.from_pretrained(MODEL_NAME)\n",
        "generation_config.max_new_tokens = 1024\n",
        "generation_config.temperature = 0.8\n",
        "generation_config.top_p = 0.95\n",
        "generation_config.do_sample = True\n",
        "generation_config.repetition_penalty = 1.15\n",
        "\n",
        "pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    return_full_text=True,\n",
        "    generation_config=generation_config,\n",
        "    pad_token_id=tokenizer.eos_token_id,\n",
        ")"
      ],
      "metadata": {
        "id": "3CRl5JadJ0Jp"
      },
      "id": "3CRl5JadJ0Jp",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = HuggingFacePipeline(pipeline=pipeline)"
      ],
      "metadata": {
        "id": "Jg5M-DFoJ5h4"
      },
      "id": "Jg5M-DFoJ5h4",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "query = \"How AWS has evolved?\"\n",
        "query = \"Who won the baseball World Series in 2020? and Who Lost\"\n",
        "result = llm(query)\n",
        "\n",
        "display(Markdown(f\"<b>{query}</b>\"))\n",
        "display(Markdown(f\"<p>{result}</p>\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "4stb186RJ-iN",
        "outputId": "c98e4c55-8629-43e6-b5d4-8eef991995b1"
      },
      "id": "4stb186RJ-iN",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "<b>Who won the baseball World Series in 2020? and Who Lost</b>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "<p> the MLB World Series?\nAnswer: The Los Angeles Dodgers won the 2020 baseball World Series, while the Tampa Bay Rays lost it.</p>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "1e4786b6-6add-4caf-9f9c-c2d8f455702a",
      "metadata": {
        "id": "1e4786b6-6add-4caf-9f9c-c2d8f455702a"
      },
      "outputs": [],
      "source": [
        "import colab_env\n",
        "import openai\n",
        "import os\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "#pack = RAGFusionPipelinePack(docs, llm=OpenAI(model=\"gpt-3.5-turbo\"))\n",
        "\n",
        "### MISTRAL\n",
        "pack = RAGFusionPipelinePack(docs, llm)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0a84473-0b2f-4649-be8e-7a603f526f02",
      "metadata": {
        "id": "b0a84473-0b2f-4649-be8e-7a603f526f02"
      },
      "source": [
        "## Inspecting the Code\n",
        "\n",
        "If we take a look at how it's setup (in your downloaded directory, you'll see the following code using our QueryPipeline syntax).\n",
        "\n",
        "`retrievers` is a dictionary mapping a chunk size to retrievers (chunk sizes: 128, 256, 512, 1024).\n",
        "\n",
        "```python\n",
        "# construct query pipeline\n",
        "p = QueryPipeline()\n",
        "module_dict = {\n",
        "    **self.retrievers,\n",
        "    \"input\": InputComponent(),\n",
        "    \"summarizer\": TreeSummarize(),\n",
        "    # NOTE: Join args\n",
        "    \"join\": ArgPackComponent(),\n",
        "    \"reranker\": rerank_component,\n",
        "}\n",
        "p.add_modules(module_dict)\n",
        "# add links from input to retriever (id'ed by chunk_size)\n",
        "for chunk_size in self.chunk_sizes:\n",
        "    p.add_link(\"input\", str(chunk_size))\n",
        "    p.add_link(str(chunk_size), \"join\", dest_key=str(chunk_size))\n",
        "p.add_link(\"join\", \"reranker\")\n",
        "p.add_link(\"input\", \"summarizer\", dest_key=\"query_str\")\n",
        "p.add_link(\"reranker\", \"summarizer\", dest_key=\"nodes\")\n",
        "```\n",
        "\n",
        "We visualize the DAG below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "34143d87-163f-4246-8ee5-15940d17c629",
      "metadata": {
        "id": "34143d87-163f-4246-8ee5-15940d17c629"
      },
      "outputs": [],
      "source": [
        "from pyvis.network import Network\n",
        "\n",
        "#\"cdn_resources is not in ['in_line','remote','local'].\"\n",
        "#net = Network(notebook=True, cdn_resources=\"in_line\", directed=True)\n",
        "#net.from_nx(pack.query_pipeline.dag)\n",
        "#net.show(\"rag_dag.html\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "065ef0c4-0c9e-4611-8de4-98b2a7fe3094",
      "metadata": {
        "id": "065ef0c4-0c9e-4611-8de4-98b2a7fe3094",
        "outputId": "755d5b97-666c-49ee-8e37-29e079ce4417",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "What did the author do growing up?\n",
            "The author wrote short stories and tried programming on an IBM 1401 computer during their school days. They later got a microcomputer and started programming on it, writing simple games and a word processor. They also expressed an interest in studying philosophy in college but ended up switching to AI.\n",
            "\n",
            "\n",
            "Who is the President of the USA?\n",
            "I'm sorry, but I cannot answer that question based on the given context information.\n",
            "\n",
            "\n",
            "Who won the baseball World Series in 2020? and Who Lost\n",
            "I'm sorry, but I cannot answer that query based on the given context information.\n",
            "\n",
            "\n",
            "Anything about LIPS\n",
            "Lisp is a programming language that was originally intended as a formal model of computation, an alternative to the Turing machine. It was invented by John McCarthy and later became a programming language in the ordinary sense. Lisp has a core that is defined by writing an interpreter in itself. McCarthy's original Lisp interpreter could only interpret Lisp expressions and lacked many features of a programming language. Over time, these features were added, but not using McCarthy's original axiomatic approach. Lisp has its origins in the model of computation, which gives it a unique power and elegance compared to other languages. The distinctive thing about Lisp is that it can be used to write an interpreter for itself.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#modify By FM 11/01/2024\n",
        "\n",
        "#response = pack.run(query=\"What did the author do growing up?\")\n",
        "query0=\"What did the author do growing up?\"\n",
        "query='I bought an ice cream for 6 kids. Each cone was $1.25 and I paid with a $10 bill. How many dollars did I get back? Explain first before answering.'\n",
        "query1 = \"Who is the President of the USA?\"\n",
        "query2 = \"Who won the baseball World Series in 2020? and Who Lost\"\n",
        "query3 = 'Anything about FORTRAN'\n",
        "query4 = 'Anything about LIPS'\n",
        "query5 = 'Anything about Python'\n",
        "\n",
        "response0 = pack.run(query=query0)\n",
        "response1 = pack.run(query=query1)\n",
        "response2 = pack.run(query=query2)\n",
        "response4 = pack.run(query=query4)\n",
        "\n",
        "print()\n",
        "print(query0)\n",
        "print(str(response0))\n",
        "print()\n",
        "\n",
        "print()\n",
        "print(query1)\n",
        "print(str(response1))\n",
        "print()\n",
        "\n",
        "print()\n",
        "print(query2)\n",
        "print(str(response2))\n",
        "print()\n",
        "\n",
        "print()\n",
        "print(query4)\n",
        "print(str(response4))\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "ea4ef382-fd11-4a82-8f5d-b2c53a21c665",
      "metadata": {
        "id": "ea4ef382-fd11-4a82-8f5d-b2c53a21c665"
      },
      "outputs": [],
      "source": [
        "# response.source_nodes"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "llama_hub",
      "language": "python",
      "name": "llama_hub"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}